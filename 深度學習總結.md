# sli.do
* 請問老師，神經網路的bias為何通常初始值為0?

bias 等同於搜尋 global minimal 的起點
* 想要請問老師，在深度學習的演算法中，有沒有什麼Guideline可以判斷說這個問題大概需要幾層的神經網路？還是只能Trial and Error？謝謝

一般只能 trail and error,
* DropOut 只有在train的時候使用，但如果到了 test 的時候就沒有Dropout了，那為什麼還會有效果呢？因為我本來以為你必須要對 train data 和 test data做一樣的 feature engineering。

進行dropout的時候通常是對模型中比較弱的部分訓練
* 梯度下降時亂數決定的權重值會讓訓練時間變長嗎？ (比方說一開始離最佳解比較遠導致要跑比較多次才收斂?)

會
* 請問發現收斂時要怎麼判斷是掉到local or global min?

判斷不出來
* 在投影片 "Batch normalization -- 再變換" 這頁 老師有提到BETA和GARMA是用梯度下降學習來的 再加上原本訓練Weight時也會用梯度下降 所以這樣就會用到兩次梯度下降是嗎? 如果是，這樣會不會很花時間 & 演算法很複雜很難維護? 謝謝

不是, 不會
* ReLU這一系列的activation function在z值為負時回傳的值都是接近或等於零；請問z值為負代表的意義是什麼？為什麼要這樣處理？


* 一層hidden layer就能表達所有函式的話，為什麼要使用多層結構？這樣不是讓參數更難調整嗎？（因為單層只要調整神經元數量，多層要同時考量層數與神經元的數量）

1. 一層無法表達所有函數
2. 是
* 請問為什麼 relu 要設計成Z>0維持線性, Z<0都要維持0 如果不全部都維持線性, 像是一條從右上到左下的直線？

why not use linear?  因為這樣才會持續更新

# Q&A
* 用ai測跟用人工哪個比較ok
ai迭代比較快

