# common mistake in ml
Q: a model or data size is more important?
overfitting像是背答案
* overfitting 也許能被足夠大量的資料解決
  * random noise tends to average out
  * training data 已經夠 general 的時候
吳恩達
1. model 在 training data 上的表現(不好就加更多data)
2. model 在 testing data 上的表現(不好增加更多data)
3. 增加更多 data
hyper-parameter
K(KNN), lambda(linear / logistic)
quiz:
* 如果model在training data ok, testing data bad, 那要對 model 還是對 data 動手腳?
  * prevent under-fitting: complex model
  * prevent over-fitting: simple model or more data
* 如果 model 在 training / testing 都表現差不多的時該怎麼調整
  * 調整hyper-parameter
在大多數場景下我們只能有固定的dataset, 怎麼切分dataset?
切 training, validation, testing

如果在testing data上tune hyper-parameter, 只是overfits the test data
# 資料內有時間相關的data
用一般的切法可能會有極值影響的問題(比如說1/1因為某件事情股價特別高)\
比較合理的方式是用時間切(2013/1/1之前的當作training, 之後的當testing), 不能把未來的資訊(feature)切進 training data 中\
據循環性的time(1-7, 23:00~01:00)
* sinusoid function(取sin跟cos作為距離)\
[各種統計關聯](http://tylervigen.com/spurious-correlations)\
features >> instances 的時候通常會產生關聯(但通常是無關)
# test env is different from training
在某些情境下切資料是不夠的
* 現有的test data 也許已經無法表現未來的情境\
如果現有的資料是基於特定算法產生的, 新的分析有可能讓rich get richer, A\B test 是比較好的做法

sli.do:
* 想做降維時，用auto-encoder跟PCA各有什麼優缺點，使用時機為何?\
PCA: 允許非線性的 feature
auto-encoder: 有機會在的較低的維度還原原始的維度, 但計算成本較高
T-SNE: 無法對新的 input 作維度比較(需要重新算舊的資料), 上面兩者都可以
* 問一下 Lasso 與 PCA 都用在降維 哪時候用Lasso, 哪時候用PCA呢？
|PCA|Lasso|
|:--|:--|
|與 y 無關|與 y 有關|
|非監督|監督|
|根據 X 找到能最大程度保留 X 的 k 個方向|X 與 y 無關的 features 所對應的 theta(i) 很可能會變成 0|
* 請問 SVR 做出來的模型 要怎麼解釋每個 feature 跟預測結果的關係？\
如果是 linear SVM 可用 SGD 求解, 得到的theta i 可當作第 i 個 feature 對 y 的影響力\
如果 kernel SVM, 每個 feature 與 y 的關係難以直接衡量
* 跟05-svm.pdf page12 所提到的Linear SVM with soft margin比對後， 發現05-svm.pdf page40 所提到的linear SVM的式子少了 b這個變數 可以請老師幫忙解釋一下這個差異嗎? 謝謝\
只是為了講解跟式子表示的方便性所以把它拿掉
1. 在高維度的時候 b 影響不重要
* 05-lagrange.pdf page9, 關於 KKT condition中有提到Dual feasibility(lambda_i大於等於0) ,Complementary slackness (lambda_i*g_i=0)，請問這個部分有進一步的物理意義或解釋嗎?
* 在做feature selection時，有沒有什麼刪除feature的準則，例如相關係數低於多少就不使用？不同的model標準又有什麼不同呢？\
有幾種方式
1. 不管 y 直接做(身高與體重)
2. 增加 feature 時對預測結果是否有幫助
* 請問如在 data 很少的情況下( < 15 ), 分群是有意義的嗎
分群本身有無意義就不是很確定, 因為通常只是拿來看是否有隸屬某個群
* 請問當資料量不夠但又蒐集不到新資料的時候，可以用現有的資料做出新的資料 （ex 資料1的第一個feature＋資料2的第二個feature --> "新的"資料3） 拿來訓練嗎？ 謝謝
1. 此例子應該是不行
2. 資料量不大的時候可以用 cross validation
* 在 kaggle 提供的資料下已經無法提升model表現，可以外加官方以外的資料進來做學習，然後提交結果嗎?\
看比賽規定拉
* 老師您課堂上說不用調超參數的model可將資料分成train與test訓練，那訓練出來的模型在實際應用時，是否會因為fitting資料導致預測的效果不好?若不好，要如何調整模型的訓練方式?還是在訓練過程將資料分成train、validation與test會得到較好的效果?\
如果 model 本身不需要調整 hyper-parameter, 只切 training & test is ok
* 老師好： 剛剛上堂課有提到只要features數量多於instance，很容易遇到現實上其實不相關，但資料上正好高度相關的情形。 請問，所以現實上不相關的變數(人為判斷)就不需要餵進模型嗎？ 還是依然可以餵入呢？ 有關變數的篩選還真是大哉問啊！ 謝謝老師。\
依照 domain knowledge 判定
* 老師請問剛剛的投影片有介紹到用時間切割 train set 和 test set, 但是似乎圖顯示 train set的長度(天數)可變, 這樣 model input 該如何設計?\
已電商為例, 在非DL的情況, 可以用 過去N天平均購買單價 / 平均購買單價 之類的方式去固定 feature size
在DL的情況有些模型可以接收時間序列的data(RNN)

# recommender systems
## Netflix prize(網飛辦的ai比賽)
* factorization-based approaches
### 推薦系統類型
* content-based
依照內容相關性
* collaborative filtering
依使用者評價
user-based CF:KNN
1. 算個使用者的平均給分
2. 計算預測點與各點的 cos similarity
3. 把比較像的兩者計算平均
item-based CF:KNN
1. 計算每部片平均分數
2. 計算cos similarity
3. 把比較像的兩者計算平均
model-based CF:
https://sifter.org/~simon/journal/20061211.html
a.k.a SVD / latent factor model
SVD 的限制:\
* cold start: 新的item(user)/long-tail items沒人評分, 無法知道其 latent factor

solution:
* transfer learning

[FFM](https://tech.meituan.com/2016/03/03/deep-understanding-of-ffm-principles-and-practices.html) 模型在做推薦的表現不錯

