# GAN part1
[video1](https://www.youtube.com/watch?time_continue=61&v=oE6Xe5Cyy7Y)
[video2](https://www.youtube.com/watch?time_continue=1&v=ZBY1shLnhUk)

# concept
Generator(NN, vector to image) + Discriminator(NN, image to scalar)
# 訓練方式
交替的訓練
* step1. 固定 G 訓練 D
* step2. 固定 D 訓練 G, 嘗試騙過 D

# 另一種生成技術 variational Auto-encoder
只使用 G(NN decoder), 期望輸出的結果與輸入越接近越好
## G
G^* = arg * minofG * Div(P_G, P_data)
找出一個 G 使得 Div(P_G, P_data)越小越好
## D
D^* = arg * maxofD * V(D, G) => Div
# GAN 很難 train 的理由
* In most cases, P_G and P_data art not overlapped
* 實際上在 sampling 的時候我們並不知道 P_G, P_data 到底長怎樣
## 如何衡量
* JS divergence: 只要預測的結果不相等都等於是log2, 不準確
* Wasserstein distance: W(P, Q) = d
WGAN
曲線要夠平滑, how?
Weight Clipping(general) / Gradient Penalty(improved)

# 一些小技巧
* 提升測試時的品質
1. 縮小訓練 G 的時候 input sample 的 variance, 但可能產出結果會比較單一
2. Ensemble: 解決下面兩種方法的技巧, 使用多個 generator
Mode Collapse: 只能產出特定的結果
Mode Dropping: 只能產出特定群組的結果
# Objective Evaluation
Inception score(影像類)
Negative entropy of P(y|x)  越大越好
Entropy of P(y) 越大越好
