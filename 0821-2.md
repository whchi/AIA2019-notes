# 教材
[video](https://www.youtube.com/playlist?list=PL1f_B9coMEeAsw-Cs7WmlBfbUpxIVgvb6)

# 對抗 overfitting
1. regularization: 限制 weights 的大小

當 w 越大的時候 loss 會提升;獎勵 w 比較小的結果(從loss下手)\
當 w 可調整的範圍越大的時候, 表示此網路越有能力去 fit data\
l1-regularizer(sum of abs(values))\
l2-regularizer(RMSE)
2. early stopping

當overfitting(validation loss沒什麼進步)發生的時候就停止訓練
3. dropout

神經網路中每個layer的連結為全連結, 此方法是訓練過程中隨機關閉一些連結(把 w 設為 0)\
因為 overfitting 等同於猜太準, 把它變笨一點\
可視為一種終極的 ensemble 方法\
model ensemble: 三個臭皮匠勝過一個諸葛亮\
實作上是在每個節點的 input 上加入一個開關(白努力分佈)
# 實作 early stopping
每跑完一個epoch實察看 loss 有沒有更小, 有的話就把當下的loss更新並重設rate limit, 沒有的話就往上加直到超過門檻
# 實作 regularization
適合較淺層的NN
# 實作 dropout
適合較深層的NN
# keras

