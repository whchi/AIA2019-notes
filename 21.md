# [regularize linear regression](https://www.youtube.com/watch?v=GwLzpDmBke8&list=PL1f_B9coMEeCvbetNGYmW7fWUBSo0-D_i&index=4)

SVM: 最大化margin, 使用soft margin or kernel trick
logistic regression: 最大化log-likelihood or 最小化 cross entropy loss, 使用GA or GD
* common loss functions
1. hinge loss: y*f(x) >= 1 的時候才不會有loss
2. squared hinge loss: 跟前者類似, 但差距大的實惠會有較大的penalty
3. logistic loss:
使用 hinge loss 等於 SVM, 使用 logistic loss 等於 logistic regression

最大區別
hinge: f(x) * y >=1 no penalty
logistic: unless f(x) * y is unlimited, or always have penalty
* summary\
svm 跟 logistic regression 是類似的但因為以下原因所以被當成不同\
通常說到 SVM 都是指 kernel SVM, 但其實還是有 linear 的 SVM
通常說到 logistic regression 則為 linear, 但可以做 kernel, 不過很少用

kernel(linear) SVM , kernel(linear) logistic regression 效果差不多
