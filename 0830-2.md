# 教材
[video](https://www.youtube.com/playlist?list=PL1f_B9coMEeBtHVc9a1Z9HBUiKHqKO1Wv)

# 代表性模型介紹
## AlexNet
[貢獻](https://medium.com/@WhoYoung99/alexnet-%E6%9E%B6%E6%A7%8B%E6%A6%82%E8%BF%B0-988113c06b4b)
5 cov layer + 3 fully layer\
始祖等級地位\
主要貢獻有\
1. 使用relu
2. 使用多GPU
3. 使用LRN: 我們的視覺對於相對大小比較敏感, 絕對大小比較不敏感, 此方法縮小絕對大小而不縮小相對大小
4. 重疊的pooling運算
5. 使用data augmentation & dropout
why relu? 快速地收斂速度與解決梯度消失問題
## VGGNet
開啟深層CNN\
16 or 19層
3 * 3 cov, 2 * 2 pooling layers, 3 fully connected layers
現在很多都先用VGG try
## GoogLeNet
22 layer\
12 times lesser parameters than alexnet\
no fully connected layer, all cov layer\
inception module(nn in nn)\
auxiliary classifiers
* inception module

[1*1卷積層](https://medium.com/@chih.sheng.huang821/%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E8%B7%AF-convolutional-neural-network-cnn-1-1%E5%8D%B7%E7%A9%8D%E8%A8%88%E7%AE%97%E5%9C%A8%E5%81%9A%E4%BB%80%E9%BA%BC-7d7ebfe34b8), 只有再多channel的時候能夠有效降維(降低channel數量)\
使用 1*1 feature 有效降低 feature map 張數(每下1/2可以減少1/4個feature, 因為用1 * 1的feature做pooling運算), 將地feature map能有效降低運算時間
使用 global avg pooling 取代 fully connected layer
* auxiliary classifier(輔助分類器)
傳統DL層數多的話會有梯度消失的問題(因為BP會一直使用chain rule往回計算梯度, 越前面越多數值要連續相乘), 此方法是在網路中間穿插兩個輔助的分類器, 由該分類器做BP直到前一個分類器以減少乘數過多的問題,避免從output做BP時導致越前面的layer乘數越多
testing的時候會把輔助分類器拔掉
## ResNet
[補充](https://zhuanlan.zhihu.com/p/31852747)\
152層\
最廣泛使用\
太深層的網路通常會有梯度爆炸/消失的問題, 此net主要解決此問題
堆疊多個residual modules組合而成
the deeper, the better
1. 學習參數較少
2. 加速DNN的訓練
3. 減少梯度消失風險
4. 增加網路層數
5. 提高辨識度
## DenseNet
[補充](https://zhuanlan.zhihu.com/p/37189203)\
1. 減緩梯度消失(同resnet)
2. 增強feature傳遞
3. 增強feature reuse
4. 減少input parameter
跟resnet最大不同在於每個layer都會跟不同的layer做connection當作下一層的cov運算
但每一層的feature會過大
# Network comparison
