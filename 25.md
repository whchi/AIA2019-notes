# 非監督式學習
只根據featur預測的問題
## [dimension reduction](https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E9%99%8D%E7%B6%AD-dimension-reduction-%E7%B7%9A%E6%80%A7%E5%8D%80%E5%88%A5%E5%88%86%E6%9E%90-linear-discriminant-analysis-d4c40c4cf937)
1. compress data and preserve useful information
2. data visualization
常碰到的問題是壓縮空間成功, 但可能會有些微loss
### [PCA](https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8-%E7%B5%B1%E8%A8%88%E5%AD%B8%E7%BF%92-%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90-principle-component-analysis-pca-58229cd26e71)
把高維的投影到低維的空間上, variance越大越好
* summary
[特徵向量與特徵值](http://silverwind1982.pixnet.net/blog/post/154593170-%E7%89%B9%E5%BE%B5%E5%90%91%E9%87%8F%28eigenvector%29-%E5%8F%8A-%E7%89%B9%E5%BE%B5%E5%80%BC%28eigenvalue%29-%E7%9A%84%E5%AE%9A)
perform PCA by computing the eigenvectors of the k largest eigenvalues of the covariance matrix
### [T-SNE](https://mropengate.blogspot.com/2019/06/t-sne.html)
同PCA, 且可保留點與點之間的特性, 可作非線性轉換
goal: minimize KL-divergence of P and Q
* KL-divergence
衡量兩個 dist 的距離是大還是小
### summary
T-SNE跟PCA目的相同, PCA只能做線性轉換, T-SNE可以做非線性轉換
PCA: 可解釋性高, 可以投影新的data
T-SNE: 視覺化時效果比較好, 無法投影新的data
## [clustering](https://www.youtube.com/watch?v=AiTbVJLEhCM&list=PL1f_B9coMEeDMRTE71laJp3BYe9dOE_Ak&index=5)
similar的定義
* euclidean distance
### kmeans
1. 分 k 群
2. 隨機猜測 k個點作為群的中心
3. 看過所有資料點, 距離各個群中心最近的表示為該群
4. 計算屬於同個群的中心點在哪
5. repeat 3~4
6. repeat until terminated
### hierarchical clustering
* bottom-up: 找出最近的兩點當作一群, 在找出這群跟其他點最近的距離作為新的群, 直到剩下一群
* top-down: 假設全部點屬於一個群, 不斷分成 2 組直到最後各組都只有 1 個點, 通常需要搭配其他算法輔助分群

# [summary](https://www.youtube.com/watch?v=i4kV7BNuC0U&list=PL1f_B9coMEeDMRTE71laJp3BYe9dOE_Ak&index=10)

