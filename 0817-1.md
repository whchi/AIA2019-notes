# 教材
[video](https://www.youtube.com/playlist?list=PL1f_B9coMEeB7VvBbxn6Xz29hL-_rvtIB)
[course ppt](https://docs.google.com/presentation/d/14f_MvayqywKyzJuK55kDBRb6IEK_TFlVyXEGGxiJP1c/edit#slide=id.g546583dea6_0_12)
https://www.youtube.com/watch?v=IHZwWFHWa-w
https://www.youtube.com/watch?v=Ilg3gGewQ5U

# 梯度下降與反向傳播
大概是 x * w 獲得 y-hat, 用 loss 計算與實際結果的誤差, 用 BP 取得各層 w, 用 GD 調整梯度, 重新計算 y-hat 與loss, 重複此過程直到 loss 最小
在這個 loss 之下所有點數中可以讓斜率降低的方向前進是梯度下降\
獲得梯度: 反向傳播
反向傳播: 將loss由輸出層往輸入層輸, 逐層獲得梯度, 用梯度下降條神經元的權重\
運用連鎖率與偏微分取每層的梯度

反向傳播
first term: 輸出值對 w 做偏微分
input layer to layer1: 原始輸入
layer i-1 to layer 1: 前一層輸出
second term: error signal

learning rate 的影響: 太小會太慢, 太大會跳太遠
## mini-batch vs epoch
* epoch: 看完所有
* mini-batch: 每次看一段, 通常不會設定太大

## 梯度消失問題
停留在local minimum, 利用 momentum 克服
參考前一次的 gradient 作為 momentum
* momentum
1. 先算 gradient
2. 加上 momentum(前一次的gradient)
3. 更新

* nesterov momentum
1. 加上 momentum
2. 再算 gradient
3. 更新

# 實作 sigmoid
http://www.shuang0420.com/2016/06/26/numpy%EF%BC%8D%E7%90%86%E8%A7%A3keepdims=True/
https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8-%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92-%E5%9F%BA%E7%A4%8E%E4%BB%8B%E7%B4%B9-%E6%90%8D%E5%A4%B1%E5%87%BD%E6%95%B8-loss-function-2dcac5ebb6cb
[numpy 用 array 作為 index](https://www.brilliantcode.net/1183/numpy-tutorial-indexing-with-array-of-indices/)
[softmax-crossentropy](https://deepnotes.io/softmax-crossentropy)

# 分類問題的 cost 為 cross entropy
