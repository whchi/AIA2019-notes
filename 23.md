# ensemble methods
集成多個監督式學習的預測結果計算機率
## why it's works
三個臭皮匠
## types
* bagging: resample training data
根據不同 input data 使用同樣 algo 可得不同結果, 每棵樹獨立
* boosting: reweight training data
依據每筆training data的難易度給予對應的權重
難易度: 先train一個base learner, 預測錯的為難, 對的為簡單, 難的增加權重在訓練新的base learner, 這些要盡量預測正確, 每棵樹相依, 準確度高於 bagging
* stacking: blending weak learners
把舊的base learner的output作為feature訓練新的base learner

## [bagging](https://www.youtube.com/watch?v=tkYoWXHf1Ok&list=PL1f_B9coMEeDPl3dZ_ZmoDB1A2gjPa3hg&index=2)
* bootstrap: 抽取放回 resample
* bootstrap aggregation: 重複 n 次抽取放回訓練的動作, train出多個weak learner結合成 strong learner
* random forest = bagging + randomized feature set
1. build many DT(bagging + randomized feature set)
2. 結合每個數的預測
可平行化, 因每棵樹圍獨立

## [boosting](https://www.youtube.com/watch?v=G5sSqvOr7QA&list=PL1f_B9coMEeDPl3dZ_ZmoDB1A2gjPa3hg&index=3)
每次 iter 都會把猜錯的 weight 提升作為下一次 iter 的 input
* 每個 model 的 weight 不同
* 每一筆資料的 weight 中不同
* AdaBoost
要提供預計生成的learner數量\
有兩個weight\
alpha: 每一個 learner 的權重, 比較準的權重比較高
w: 每一筆 data 於該iter的權重, 比較準的權重比較低
每筆data猜錯時在下一個iter會提升其weight
* gradient boosting: adaboost的延伸
通常f(x)是很淺的DT\
vs random forest: \
random forest的樹都是獨立生成的, gradient boosting則相#依, 無法平行產生

## stacking
另外 train 一個 model, 把前面 train 出來的 output 作為feature train, train 該 model的 training data 不可以跟先前的 model 一樣

# random forest(short for RF)
[video](https://www.youtube.com/watch?v=PwurDaxAAMk&list=PL1f_B9coMEeDPl3dZ_ZmoDB1A2gjPa3hg&index=6)\
why better than DT?\
因為用了 ensemble 的方式強化 weak learner

# [Gradient Boosting Machine](https://www.youtube.com/watch?v=wrlQsHFS9tg&list=PL1f_B9coMEeDPl3dZ_ZmoDB1A2gjPa3hg&index=8)
[GBM in sklearn](https://www.youtube.com/watch?v=cj3WRIEL0O8&list=PL1f_B9coMEeDPl3dZ_ZmoDB1A2gjPa3hg&index=10)\
[導讀](https://www.youtube.com/watch?v=KTbMRNulvZ8&list=PL1f_B9coMEeDPl3dZ_ZmoDB1A2gjPa3hg&index=9)
http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/

[XGBoost](https://www.youtube.com/watch?v=X239G9qys60&list=PL1f_B9coMEeDPl3dZ_ZmoDB1A2gjPa3hg&index=11)
GBM implementation
* additive model(相依生成樹)
* feature sampling(隨機抽樣feature與data)
* add regularization in object function to avoid overfitting(l1, l2)
* use 1st and 2nd derivative to help training(生成下一棵樹)

earlystop: 保留在 validation loss 開始上升的之前的資料以建立較好的model
