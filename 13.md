# Regression
## common used sign
|sign|mean|
|:--|:--|
|x|features|
|y|target|
|y-hat|prediction|
|θ|parameters (to be learned)ml要找到的參數, 根據此參數與x預測出的y-hat要趨近y|

## definition
y-hat = θ0([y截距](https://zh.wikipedia.org/wiki/%E6%88%AA%E8%B7%9D))_+θ1(斜率)x
x = [1 x]T
θ = [θ0 θ1]T

線性回歸的模型是一條線, 誤差為x經過model到y-hat的距離總和

## simple linear regression
只有一個 x 和 y pair 的時候
通常使用 [sum of squared error (SSE)](https://hlab.stanford.edu/brian/error_sum_of_squares.html)作為objective function
目的是找出 yi-hat 跟 yi 之間距離最小值
多個 x 的時候 (y-hat - y)T(y-hat - y)等同於計算SSE
常見有closed form跟梯度下降法計算
* 梯度下降
[公式推導](https://medium.com/@chih.sheng.huang821/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E5%9F%BA%E7%A4%8E%E6%95%B8%E5%AD%B8-%E4%BA%8C-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95-gradient-descent-406e1fd001f)
概念
1. 先隨機設定 theta0跟theta1的值
2. 移動兩者使得J()變小
3. 不斷重複2
but how to move? 梯度下降
alpha: 超參數(hyper parameter) a.k.a learning rate, 0 < alpha < 1
常見的調整是隨步驟增加降低alpha
可能結束的時機
1. 進步幅度趨緩
2. training的error已經很小了
3. 抵達步驟的最後一個iter
4. 沒時間訓練了
## closed form vs gradient descent
以時間和空間雜度來看\
當feature數量大的時候建議使用gradient descent\
大部分的問題沒有公式解, closed form不常見

若都是分類問題可以用線性迴歸模型解嗎?
取決於我們是否能夠encode分類的feature, 比如說用性別預測身高就不適合(M:0, F:1)

## [one-hot encode](https://medium.com/jameslearningnote/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC2-4%E8%AC%9B-%E8%B3%87%E6%96%99%E5%89%8D%E8%99%95%E7%90%86-missing-data-one-hot-encoding-feature-scaling-3b70a7839b4a)
如果要用線性模型計算分類問題, 需要做這個
為何不用一般的encoding? 舉例來說, 假如把國籍用一般的encoding為0,1,2, 則y-hat差距會被影響到失真

## overfitting
學出來的絕對值很大的時候通常是

做 [regularized regression](https://www.jamleecute.com/regularized-regression-ridge-lasso-elastic/) 避免 overfitting
* Ridge(l2-norm: 平方和)
大部分的feature會對結果有影響時使用
* Lasso(l1-norm: 絕對值和)
部分feature對結果會有明顯的影響時使用
* Elastic-net
combine ridge & lasso
為何lasso會把一部分的feature縮成0但ridge不會
1. 從幾何圖形上理解: SSE的等高線圖與 lasso(菱形) 的交叉點比起和 ridge(圓形) 的交叉點更容易交會在0的地方
2. 從數值上解釋: ridge在把大的變小的時候變化好處較明顯, 不會把小的變成0

## GD on large dataset
* stochastic GD: 不同於傳統每次都看所有, 改為每次iter的時候只看一筆training data
* mini-batch GD: 每次看 b 筆資料
## pros and cons of SGD
* pros
資料量很大, 很多redundant資料的時候比較快
支持線上(feature、target會隨時間變動)學習
有時候能夠pass local minimum
* cons
到達 global minimum 時有可能會持續移動

## summary
* 非線性迴歸可以用人工產生線性feature(label)達到線性回歸的效果: 把原有feature做數學處理視為新的feature
* overfitting 可以用 ridge, lasso, elastic net避免
* convex function
  * local min === global min
  * theta-k+1 = theta-k - alpha * g

## 評估 model 的好壞
* MSE / RMSE(把MSE做開根號)
  * 批評
    * not a normalized measure(意義不明確)
    * 會被極端值影響
* MAE
  * lower than MSE
  * 批評
    * not a normalized measure
    * 容易被 outliers(誤差值很大的結果) 影響
* MedAE
  * 不易受極端值影響
  * 批評
    * not a normalized measure
* [R^2 score(coefficient of determination)](https://zh.wikipedia.org/wiki/%E5%86%B3%E5%AE%9A%E7%B3%BB%E6%95%B0)
