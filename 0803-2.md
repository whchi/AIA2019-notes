# 教材
* [ppt](https://docs.google.com/presentation/d/1DF9RzA0XyxAdSw-DhUrMs7WsMBl70aMQRChGroRNoXM/edit#slide=id.p1)
* [video](https://www.youtube.com/playlist?list=PL1f_B9coMEeAq0pcCCMx0UoxRkQ392hy2)
* [14](14.md)
## data split: 切分training & testing data
```py
from sklearn.model_selection import train_test_split
xTrain, xTest, yTrain, yTest = train_test_split(x, y, test_size=0.33, random_state=42, shuffle=False)
# random_state: only works while shuffle=True
```
## normalization
常用的是scale
* why need norm?
1. 提升預測準度
2. 提升模型效率
## one-hot encoding
```py
# unordered
# ordered: 自定義順序再map
```
## 評估模型
mae、mse、rmse、sr2-score
## f-score
recall、precision
## prevent overfitting
模型參數(coefficient)越多, 越容易用不相關的參數去fitting資料雜訊
* weight regularization
weight是theta(coefficient)
ridge(l2)
lasso(l1)
## summary
alpha越大對高次項(不相關的features)的縮限越嚴謹, 抵抗雜訊能力越強
lasso會產生較多0的coefficent
