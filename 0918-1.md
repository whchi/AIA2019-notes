# RNN
[video](https://www.youtube.com/playlist?list=PL1f_B9coMEeAaZ1-Cgakm4HeYx6YTE70R)

# CNN
看過一個特徵, 掃過每個點, 紀錄該點附近的特徵強度
# RNN
上一次的輸出當作下一次的輸入\
同層的前一層output(hidden state)跟上層的output都會輸入\
[輸出與狀態的差別](https://zhuanlan.zhihu.com/p/28919765)
* 對話機器人
每次輸入跟輸出都不是固定長度
## seq to seq
可以描述一段影片發生的事情
## 情意分析
f(comment) = 1(positive) or 0(negative)
## slot filling
分詞並歸類
不管哪種類型結構都一樣, 不過只取要的部分
* one2many: 生成文章
* many2one: 情意分析
* many2many: 對話機器人、slot filling
## bidirection RNN
正向、反向各做一次(因為前幾層預測的能力很差, 兩個方向都做能提高成果), 讓 RNN 效果更佳
* [BPTT(back propagation through time)](https://zhuanlan.zhihu.com/p/26892413)
## 如何解決梯度消失
* [LSTM](https://zhuanlan.zhihu.com/p/32085405)
多一個cell state留給cell本身
Gate: 0 ~ 1 的數字
使用 3 個 gate(忘記門f, 輸入門i, 輸出門o)
tanh: -1~1的數值
sigmoid: 0~1的值

本次的c = f * 前一次產生的c + i * 新的c(tanh(前面的input w * x + 前面的hidden w * h + b))
本次的h = o * tanh(c)
* GRU
只用兩個 gate(記憶門z(同忘記門)、重設門r)
