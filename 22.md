# [decision tree](https://www.youtube.com/watch?v=8MR5sRyd6zQ&list=PL1f_B9coMEeDnlocZvO4vREgupj3TWhh5&index=2&t=0s)

根據訓練資料自動產生一個if-else的樹
左側是true, 右側是false
## 如何產生
根據訓練資料自動產生一棵樹
以 greedy 的方式決定, 找到一個 attr 使得每一群都切的很開
* [information gain(ID3)](https://www.youtube.com/watch?v=w_zsmInRBlw&list=PL1f_B9coMEeDnlocZvO4vREgupj3TWhh5&index=2)
entropy: 衡量一個分部有多亂, 一半一半是很亂的
問題: 可能會選擇values比較多的attribute, 比如說使用id作為attribute
* gain ratio(C4.5): a norm to information gain\
Gain / SplitInfo
* gini index(CART)\

三者結果差不多

## overfitting and tree pruning(防止)
理論上不限制的話預測率是100%(leaf nodes === n)
1. pre-prune the tree
   1. 最高多高
   2. 低於某個值就不要再長
規則簡單, 計算量小, 不同類型訓練資料限制的方式應該要不同
2. post-prune the tree
計算時間較長, 能夠根據資料特性改變樹的形狀
砍樹的時候 training data 的錯誤理論上會比較高, 使用alpha推斷有沒有砍對(越小越好)
* 樹越小越好(規則簡單)
* 用training data 的 error-rate 低, 與前者相衝突
* alpha = (err after cut - err before cut) / leaves been cut - 1
## [注意事項](https://www.youtube.com/watch?v=qICHitE2Dks&list=PL1f_B9coMEeDnlocZvO4vREgupj3TWhh5&index=10)
是數值形 feature 時如何做?\
不會把數值本身當作一組(因為可能每組數值不同), 而是先分組, 比如說年紀/身高\
因為是greedy方法, 所以結果不見得是好的樹, 通常認為小的樹比較好
* [奧卡姆剃刀原則](https://zh.wikipedia.org/wiki/%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80)

## regression tree
RSS(residual sum of squares), max-min, variance盡量小, 不斷重複
## summary
* 使用training data建立 分類/回歸rules
* interpretable prediction(可解釋的預測, 可回推)
quiz: 在全數值的training data中, 哪個預測需要花最多時間? KNN/logistic regression/dt
KNN: 需要看完所有的data後再計算鄰居判斷其label
logistic: 訓練完就知道每個feature對預測結果的重要度, 在features不多的話很快
dt: 同一個feature可能被看很多次, 只需要樣本本身,  不用看所有data
